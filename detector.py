import math
import os
import numpy as np
import tensorflow as tf
from tensorflow.python.ops.rnn_cell import GRUCell
from tensorflow.python.ops.rnn_cell import LSTMCell
from tensorflow.nn.rnn_cell import BasicRNNCell
from tensorflow.python.ops.rnn_cell import MultiRNNCell
from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper
import tensorflow.contrib.layers as layers
from tensorflow.python.layers.core import Dense
from tensorflow.python.framework import dtypes
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc, accuracy_score,recall_score,precision_score
import data.util as utils
from utils.utils import cal_roc,savefig
from data.data_utils import prepare_batch,malware_label,benign_label
import json
class Detector(object):
    def __init__(self, config, mode):
      with tf.variable_scope('detector',reuse=tf.AUTO_REUSE):
        self.config = config
        self.mode = mode.lower()
        self.cell_type = config['cell_type']
        self.depth = config['depth']
        self.is_birnn = config['birnn']
        self.class_num = config['class_num']
        self.dtype = tf.float16 if config['use_fp16'] else tf.float32
        self.voc = config['voc']
        self.use_residual = config['use_residual']
        self.use_dropout = config['use_dropout']
        self.batch_size = config['batch_size']
        self.hidden_units = config['hidden_units']#len(config['voc'])#config['hidden_units']
        self.optimizer = config['optimizer']
        self.learning_rate = config['learning_rate']
        self.stride = config['stride']
        self.max_gradient_norm = config['max_gradient_norm']
        self.maxlen = config['maxlen']
        self.keep_prob_placeholder = config['keep_prob_placeholder']
        self.embeddings = tf.get_variable(initializer=tf.one_hot(self.voc, len(self.voc), dtype=self.dtype), name='emmbedding', trainable=False)
        #self.embedding_arg
        self.global_step = tf.Variable(0, trainable=False, name='global_step')

        if config['model'].lower() == 'rnn':
            self.build_model()
        elif config['model'].lower() == 'att1':
            self.build_model_att1()
        elif config['model'].lower() == 'att2':
            self.build_model_att2()
        elif config['model'].lower() == '2att':
            self.build_model_2att()

        if self.mode == 'pretrain':
            self.init_optimizer()
        elif self.mode == 'test':
            pass

        tf.summary.scalar('loss', self.total_loss)
        tf.summary.scalar('accuracy', self.accuracy)
        self.summary_op = tf.summary.merge_all()

    def task_specific_attention(self, inputs):
        with tf.variable_scope('attention') as scope:
            attention_context_vector = tf.get_variable(name='attention_context_vector', shape=[self.hidden_units,1],  dtype=self.dtype)
            #return tf.reshape(tf.matmul(tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(tf.tanh(inputs), [self.maxlen*self.total_num, self.hidden_units]),attention_context_vector),[self.total_num, self.maxlen])),[self.total_num,1, self.maxlen]), inputs),[self.total_num, self.hidden_units])
            return tf.reshape(tf.matmul(tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(tf.tanh(inputs), [-1, self.hidden_units]),attention_context_vector),[self.total_num, -1])),[self.total_num,1, -1]), inputs),[self.total_num, self.hidden_units])

    def task_total_attention(self, attention_r, mode):
        with tf.variable_scope('attention') as scope:
            sen_d = tf.get_variable('bias_d', [self.class_num])
            initial_outputs = tf.TensorArray(dtype=self.dtype, size=self.batch_size)
            def __logit__(x):
               with tf.variable_scope('attention_logits', reuse = True):
                   relation_embedding = tf.get_variable('relation_embedding', [self.class_num, self.hidden_units], initializer = tf.contrib.layers.xavier_initializer())
               return tf.matmul(x, tf.transpose(relation_embedding)) + sen_d
            def __attention_train_logit__(i, out):
               _cur = tf.gather(self.total_shape, i)
               _next = tf.gather(self.total_shape, i+1)
               bag_hidden_mat = attention_r[_cur: _next]
               attention_score = tf.nn.softmax(tf.reshape(attention_logit[_cur:_next],[1, -1]))
               out = out.write(i, tf.squeeze(tf.matmul(attention_score, bag_hidden_mat)))
               return i+1, out
            def __attention_test_logit__(i, out):
               _cur = tf.gather(self.total_shape, i)
               _next = tf.gather(self.total_shape, i+1)
               bag_hidden_mat = attention_r[_cur: _next]
               attention_score = tf.nn.softmax(tf.transpose(attention_logit[_cur:_next,:]))
               out = out.write(i, tf.diag_part(tf.nn.softmax(__logit__(tf.matmul(attention_score,bag_hidden_mat)),-1)))
               return i+1, out
            cond = lambda t, *args: t < self.batch_size
            with tf.variable_scope('attention_logits', reuse = False):
               relation_embedding = tf.get_variable('relation_embedding', [self.class_num, self.hidden_units],initializer = tf.contrib.layers.xavier_initializer())
            if mode == "pretrain":
               if self.use_dropout:
                  tf.contrib.layers.dropout(attention_r, self.keep_prob_placeholder)
               current_rel = tf.nn.embedding_lookup(relation_embedding, self.query)
               attention_logit = tf.reduce_sum(current_rel * attention_r, -1)
               t, outputs = tf.while_loop(cond, __attention_train_logit__, [0, initial_outputs])
               sen_out = outputs.stack()
               return __logit__(sen_out)
            else:
               attention_logit = tf.matmul(attention_r, tf.transpose(relation_embedding))
               t, outputs = tf.while_loop(cond, __attention_test_logit__, [0, initial_outputs])
               sen_out = outputs.stack()
               return sen_out

    def cal_loss(self, logits):
        pred = tf.argmax(logits, axis = 1)
        cond = lambda t, *args: t < self.batch_size
        initial_outputs = tf.TensorArray(dtype=tf.int32, size=self.batch_size)
        initial_logit = tf.TensorArray(dtype=self.dtype, size=self.batch_size)
        def __at_least_one__(i, out, logit):
            _cur = tf.gather(self.total_shape, i)
            _next = tf.gather(self.total_shape, i+1)
            _pred = pred[_cur: _next]
            countTrue = tf.cast(tf.count_nonzero(_pred), tf.int32)
            logit = logit.write(i, tf.reduce_mean(logits[_cur: _next], 0))
            out = out.write(i, tf.cast(tf.equal(countTrue, (_next - _cur)), tf.int32))
            return i+1, out, logit
        t, outputs, logits = tf.while_loop(cond, __at_least_one__, [0, initial_outputs, initial_logit])
        self.prediction = outputs.stack()
        self.logits = logits.stack()
        self.total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels))
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(self.prediction, tf.int32),self.labels), self.dtype))

    def rnn(self):
        self.inputs_embedded = tf.nn.embedding_lookup(params=self.embeddings, ids=self.inputs)
        cell_forward = self.build_cells()
        outputs, state = tf.nn.dynamic_rnn(cell=cell_forward,
                                               inputs=self.inputs_embedded,
                                               dtype=self.dtype, time_major=False)
        return outputs

    def birnn(self):
        self.inputs_embedded = tf.nn.embedding_lookup(params=self.embeddings, ids=self.inputs)
        cell_forward = self.build_cells()
        cell_backward = self.build_cells()
        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_forward, cell_backward, self.inputs_embedded, dtype=self.dtype, time_major=False)
        #return tf.concat(outputs,2)
        fw, bw = outputs
        return tf.add(fw, bw)

    def build_model(self):
        print('create model!')
        self.init_placeholders()
        if self.is_birnn:
             outputs = self.birnn()
        else:
             outputs = self.rnn()
        outputs = tf.reshape(outputs, [self.total_num, self.maxlen*self.hidden_units])
        output_layer = Dense(self.class_num)
        self.logits = output_layer(outputs)
        self.cal_loss(self.logits)

    def build_model_att1(self):
        print('create model!')
        self.init_placeholders()
        if self.is_birnn:
             outputs = self.birnn()
        else:
             outputs = self.rnn()

        attention_r = self.task_specific_attention(outputs)
        output_layer = Dense(self.class_num)
        self.logits = output_layer(attention_r)
        self.cal_loss(self.logits)

    def build_model_att2(self):
        print('create model!')
        self.init_placeholders()
        if self.is_birnn:
             outputs = self.birnn()
        else:
             outputs = self.rnn()
        output_layer = Dense(self.hidden_units)
        attention_r = output_layer(tf.reshape(outputs,[self.total_num, self.maxlen*self.hidden_units]))
        sen_out = tf.cond(self.is_train, lambda: self.task_total_attention(attention_r, "pretrain"), lambda: self.task_total_attention(attention_r, "test"))
        self.logits = tf.nn.softmax(sen_out)
        self.prediction = tf.argmax(self.logits, axis=1)
        self.total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=sen_out, labels=self.labels))
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(self.prediction, tf.int32),self.labels), self.dtype))

    def build_model_2att(self):
        print('create model!')
        self.init_placeholders()
        # API features
        if self.is_birnn:
             outputs = self.birnn()
        else:
             outputs = self.rnn()
        # Window level att
        attention_r = self.task_specific_attention(outputs)
        # Sequence level att
        sen_out = tf.cond(self.is_train, lambda: self.task_total_attention(attention_r, "pretrain"), lambda: self.task_total_attention(attention_r, "test"))
        # output layer
        self.logits = tf.nn.softmax(sen_out)
        self.prediction = tf.argmax(self.logits, axis=1)
        self.total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=sen_out, labels=self.labels))
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(self.prediction, tf.int32),self.labels), self.dtype))

    def init_optimizer(self):
        print("setting optimizer..")
        # Gradients and SGD update operation for training the model
        trainable_params = tf.trainable_variables()
        if self.optimizer.lower() == 'adadelta':
            self.opt = tf.train.AdadeltaOptimizer(learning_rate=self.learning_rate)
        elif self.optimizer.lower() == 'adam':
            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)
        elif self.optimizer.lower() == 'rmsprop':
            self.opt = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate)
        else:
            self.opt = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)
        gradients = tf.gradients(self.total_loss, trainable_params)
        # Clip gradients by a given maximum_gradient_norm
        clip_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)
        # Update the model
        self.updates = self.opt.apply_gradients(
            zip(clip_gradients, trainable_params), global_step=self.global_step)

    def init_placeholders(self):
        print('init_placeholders')
        self.inputs = tf.placeholder(dtype=tf.int32, shape=(None, None), name='inputs')
        self.labels = tf.placeholder(dtype=tf.int32, shape=(None), name='labels')
        self.query = tf.placeholder(dtype=tf.int32, shape=(None), name='query')
        self.total_shape = tf.placeholder(dtype=tf.int32, shape=(None), name='total_shape')
        self.is_train = tf.placeholder(dtype=tf.bool)
        self.batch_size = tf.shape(self.labels)[0]
        self.total_num = self.total_shape[-1]

    def build_single_cell(self):
        cell_type = LSTMCell
        if (self.cell_type.lower() == 'gru'):
            cell_type = GRUCell
        elif (self.cell_type.lower() == 'rnn'):
            cell_type = BasicRNNCell
        cell = cell_type(self.hidden_units)
        if self.use_dropout:
            cell = DropoutWrapper(cell, dtype=self.dtype,
                                  output_keep_prob=self.keep_prob_placeholder,)
        if self.use_residual:
            cell = ResidualWrapper(cell)
        return cell

    def build_cells(self):
        return MultiRNNCell([self.build_single_cell() for i in range(self.depth)])

    def check_feeds(self, inputs, label) :
        def init():
           return {},[],[],[],0,[]
        input_feed, seqs, _label, query,  total_num, total_shape = init()
        for bag,_l in zip(inputs, label):
            total_shape.append(total_num)
            total_num += len(bag)
            for seq in bag:
                seqs.append(seq)
                query.append(_l)
            _label.append(_l)
        total_shape.append(total_num)
        input_feed[self.inputs.name] = np.array(seqs)#inputs
        input_feed[self.total_shape.name] = np.array(total_shape)
        input_feed[self.labels.name] = np.array(_label)
        input_feed[self.query.name] = np.array(query)
        return input_feed

    def pretrain(self, sess, inputs, label):
        input_feed = self.check_feeds(inputs, label)
        input_feed[self.is_train.name] = True
        output_feed = [self.total_loss, self.accuracy, self.prediction, self.logits, self.summary_op, self.updates, self.labels, self.prediction]
        self.use_dropout = True
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def eval(self, sess, inputs, label):
        input_feed = self.check_feeds(inputs, label)
        input_feed[self.is_train.name] = False
        output_feed = [self.total_loss, self.accuracy, self.prediction, self.logits]
        self.use_dropout = True
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def predict(self, sess, inputs, label):
        input_feed = self.check_feeds(inputs, label)
        input_feed[self.is_train.name] = False
        output_feed = [self.prediction, self.logits, self.accuracy, self.total_loss]
        self.use_dropout = True
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def save(self, sess, path, var_list=None, global_step=None):
        # var_list = None returns the list of all saveable variables
        saver = tf.train.Saver(var_list)

        # temporary code
        #del tf.get_collection_ref('LAYER_NAME_UIDS')[0]
        save_path = saver.save(sess, save_path=path, global_step=global_step)
        #print('model saved at %s' % save_path)

    def restore(self, sess, save_path):
        ckpt = tf.train.get_checkpoint_state(save_path)
        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
             print('restore model!')
             saver = tf.train.Saver()
             saver.restore(sess, ckpt.model_checkpoint_path)
        else:
            if not os.path.exists(save_path):
                os.makedirs(save_path)
            sess.run(tf.global_variables_initializer()) 

    def restore_specific(self, sess, save_path):
        saver = tf.train.Saver()
        saver.restore(sess, save_path)
        #print('restore specific')
 
def define():
     config = {}
     config['cell_type'] = 'gru'
     config['birnn'] = False
     config['depth'] = 1
     config['model'] = '2att'
     config['save_path'] = 'model/detector/'+config['cell_type']+'_'+str(config['birnn'])+'_'+config['model']+'/'
     config['source_vocabulary'] = 'data_windows/voc.json'
     config['use_fp16'] = False
     config['class_num'] = 2
     config['use_dropout'] = True
     config['input'] = 'data_windows/train.seq'
     config['valid'] = 'data_windows/valid.seq'
     config['voc'] = utils.load_voc(config['source_vocabulary'])
     config['hidden_units'] = 128
     config['maxlen']=600
     config['stride']=300
     config['batch_size'] = 50
     config['max_batch']=800
     config['use_residual'] = False
     config['optimizer'] = 'adam'
     config['learning_rate'] = 0.001
     config['max_gradient_norm'] = 1.0
     config['max_epochs'] = 500
     config['keep_prob_placeholder'] = 0.5
     return config

config = define()
def predict(config):
     tf.reset_default_graph()
     from data.data_iterator import TextIterator, Butian_TextIterator
     valid_set = TextIterator(source=config['valid'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'])

     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
       model = Detector(config, 'test')
       model.restore(sess, config['save_path'])
       #model.restore_specific(sess, config['save_path'])
       _acc = 0
       _loss = 0
       _num = 0
       prediction = []
       all_labels = []
       all_pred = []
       for idx, sources in enumerate(valid_set):
              source_seq = sources[0]
              label = sources[1]
              sources, labels = prepare_batch(source_seq, label,max_batch=config['max_batch'], maxlen=config['maxlen'],stride = config['stride'],batch_size=config['batch_size'])
              for source,label in zip(sources, labels):
                 pred, logit, acc, loss= model.predict(sess, source, label)
                 prediction.extend(logit)
                 #all_labels.extend(label_binarize(label,classes=["0","1","2"]))
                 all_labels.extend(list(map(int,label)))
                 all_pred.extend(list(map(int,pred)))
                 #print("step {}, size {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), pred.shape, acc, loss))
                 _acc += acc * pred.shape[0]
                 _loss += loss * pred.shape[0]
                 _num += pred.shape[0]
       print(config['save_path'])
       print("step {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), _acc/_num, _loss))
       prediction = np.stack(prediction)
       all_labels = np.stack(all_labels)
       all_pred = np.stack(all_pred)
       return prediction, label_binarize(all_labels,classes=[0,1,2]), [accuracy_score(all_labels, all_pred),precision_score(all_labels, all_pred),recall_score(all_labels, all_pred)]

def save(sess, config, model):
     print(config['save_path'])
     checkpoint_path = os.path.join(config['save_path'], 'detector')
     model.save(sess, checkpoint_path, global_step=model.global_step)
     json.dump(model.config,open('%s-%d.json' % (checkpoint_path, model.global_step.eval()), 'w'),indent=2)

config = define()
maxs = 0.97
import random
def train(config, maxs):
     check = False
     tf.reset_default_graph()
     from data.data_iterator import TextIterator,Butian_TextIterator
     test_set = TextIterator(source=config['input'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'],
                            shuffle_each_epoch=True)
     valid_set = TextIterator(source=config['valid'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'],
                            shuffle_each_epoch=False)
     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
       model = Detector(config, 'pretrain')
       #model.restore(sess, config['save_path'])
       model.restore_specific(sess, '/home/dbtest/lan/Malware/model/detector/gru_False_2att/detector-9650')
       for epoch_idx in range(config['max_epochs']):
         #print(epoch_idx)
         for idx, train_sources in enumerate(test_set):
              source_seq = train_sources[0]
              label = train_sources[1]
              sources, labels = prepare_batch(source_seq, label,max_batch=config['max_batch'], maxlen=config['maxlen'],stride = config['stride'],batch_size=config['batch_size'])
              for source,label in zip(sources, labels):
                 loss, acc, pred, logit, summary, _, _labels, _1= model.pretrain(sess, source, label)
                 #print("step {}, size {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), pred.shape, acc, loss))
                 if (model.global_step.eval() % 50 == 0):
                     _acc = 0
                     _loss = 0
                     _num = 0
                     for idx, test_sources in enumerate(valid_set):
                            sub_source_seq = test_sources[0]
                            sub_label = test_sources[1]
                            sub_sources, sub_labels = prepare_batch(sub_source_seq, sub_label,max_batch=config['max_batch'], maxlen=config['maxlen'],stride = config['stride'],batch_size=config['batch_size'])
                            for sub_source,sub_label in zip(sub_sources, sub_labels):
                               pred, logit, acc, loss= model.predict(sess, sub_source, sub_label)
                               #print("step {}, size {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), pred.shape, acc, loss))
                               _acc += acc * pred.shape[0]
                               _loss += loss * pred.shape[0]
                               _num += pred.shape[0]
                     print("acc {:g}, softmax_loss {:g}".format(_acc/_num, _loss/_num))
                     #print("step {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), _acc/_num, _loss/_num))
                     if _acc/_num > maxs:
                         save(sess, config, model)
                         maxs = _acc/_num

import sys
if __name__ == '__main__':
     types = sys.argv[1]
     if types == 'train':
         train(config, maxs)
     else:
         pred,label,acc = predict(config)
         print(acc)
