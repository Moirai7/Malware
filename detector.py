import math
import os
import numpy as np
import tensorflow as tf
from tensorflow.python.ops.rnn_cell import GRUCell
from tensorflow.python.ops.rnn_cell import LSTMCell
from tensorflow.python.ops.rnn_cell import MultiRNNCell
from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper
import tensorflow.contrib.layers as layers
from tensorflow.python.layers.core import Dense
from tensorflow.python.framework import dtypes
import data.util as utils
from data.data_utils import prepare_batch
import json

class Detector(object):
    def __init__(self, config, mode):
      with tf.variable_scope('detector',reuse=tf.AUTO_REUSE):
        self.config = config
        self.mode = mode.lower()
        self.cell_type = config['cell_type']
        self.depth = config['depth']
        self.class_num = config['class_num']
        self.dtype = tf.float16 if config['use_fp16'] else tf.float32
        self.voc = config['voc']
        self.use_residual = config['use_residual']
        self.use_dropout = config['use_dropout']
        self.batch_size = config['batch_size']
        self.hidden_units = config['hidden_units']#len(config['voc'])#config['hidden_units']
        self.optimizer = config['optimizer']
        self.learning_rate = config['learning_rate']
        self.stride = config['stride']
        self.max_gradient_norm = config['max_gradient_norm']
        self.maxlen = config['maxlen']
        self.keep_prob_placeholder = config['keep_prob_placeholder']
        self.embeddings = tf.get_variable(initializer=tf.one_hot(self.voc, len(self.voc), dtype=self.dtype), name='emmbedding', trainable=False)
        self.global_step = tf.Variable(0, trainable=False, name='global_step')
        
        self.build_model_2att()
    
        if self.mode == 'pretrain':
            self.init_optimizer()
        elif self.mode == 'test':
            pass
        
        tf.summary.scalar('loss', self.total_loss)
        tf.summary.scalar('accuracy', self.accuracy)
        self.summary_op = tf.summary.merge_all()

    def task_specific_attention(self, inputs):
        with tf.variable_scope('attention') as scope:
            attention_context_vector = tf.get_variable(name='attention_context_vector', shape=[self.hidden_units,1],  dtype=self.dtype)
            #return tf.reshape(tf.matmul(tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(tf.tanh(inputs), [self.maxlen*self.total_num, self.hidden_units]),attention_context_vector),[self.total_num, self.maxlen])),[self.total_num,1, self.maxlen]), inputs),[self.total_num, self.hidden_units])
            return tf.reshape(tf.matmul(tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(tf.tanh(inputs), [-1, self.hidden_units]),attention_context_vector),[self.total_num, -1])),[self.total_num,1, -1]), inputs),[self.total_num, self.hidden_units])


    def task_total_attention(self, attention_r, mode):
        with tf.variable_scope('attention') as scope:
            sen_d = tf.get_variable('bias_d', [self.class_num])
            relation_embedding = tf.get_variable('relation_embedding', [self.class_num, self.hidden_units])
            initial_outputs = tf.TensorArray(dtype=self.dtype, size=self.batch_size)
            def __logit__(x):
               return tf.matmul(x, tf.transpose(relation_embedding)) + sen_d

            def __attention_train_logit__(i, out):
               _cur = tf.gather(self.total_shape, i)
               _next = tf.gather(self.total_shape, i+1)
               bag_hidden_mat = attention_r[_cur: _next]
               attention_score = tf.nn.softmax(tf.reshape(attention_logit[_cur:_next],[1, -1]))
               out = out.write(i, tf.squeeze(tf.matmul(attention_score, bag_hidden_mat)))
               return i+1, out

            def __attention_test_logit__(i, out):
               _cur = tf.gather(self.total_shape, i)
               _next = tf.gather(self.total_shape, i+1)
               bag_hidden_mat = attention_r[_cur: _next]
               attention_score = tf.nn.softmax(tf.transpose(attention_logit[_cur:_next,:]))
               out = out.write(i, tf.diag_part(tf.nn.softmax(__logit__(tf.matmul(attention_score,bag_hidden_mat)),-1)))
               return i+1, out

            cond = lambda t, *args: t < self.batch_size
            if mode == "pretrain":
               current_rel = tf.nn.embedding_lookup(relation_embedding, self.query)
               attention_logit = tf.reduce_sum(current_rel * attention_r, -1)
               t, outputs = tf.while_loop(cond, __attention_train_logit__, [0, initial_outputs])
               sen_out = outputs.stack() 
               return __logit__(sen_out)
            else:
               attention_logit = tf.matmul(attention_r, tf.transpose(relation_embedding))
               t, outputs = tf.while_loop(cond, __attention_test_logit__, [0, initial_outputs])
               sen_out = outputs.stack() 
               return sen_out


    def cal_loss(self, logits):
        self.prediction = []
        loss = []
        for i in range(self.batch_size):
            lens = self.total_shape[i + 1] - self.total_shape[i]
            _logits = tf.slice(logits, [self.total_shape[i], 0], [lens,-1])
            loss.append(tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits= _logits, labels= tf.fill([lens], self.labels[i]))))
            self.prediction.append(tf.cast(tf.reduce_max(tf.argmax(tf.nn.softmax(_logits), axis=1)),dtype=tf.int32))
        self.total_loss = tf.reduce_mean(loss)
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.labels, self.prediction), self.dtype))

    def build_model(self):
        print('create model!')
        self.init_placeholders()
        self.inputs_embedded = tf.nn.embedding_lookup(params=self.embeddings, ids=self.inputs)
        
        cell_forward = self.build_cells()
        outputs, state = tf.nn.dynamic_rnn(cell=cell_forward,
                                               inputs=self.inputs_embedded,swap_memory=True ,
                                               dtype=self.dtype, time_major=False)
        outputs = tf.reshape(outputs, [self.total_num, self.maxlen*self.hidden_units])
        output_layer = Dense(self.class_num)
        self.logits = output_layer(outputs)
        self.cal_loss(self.logits)


    def build_model_att(self):
        print('create model!')
        self.init_placeholders()
        self.inputs_embedded = tf.nn.embedding_lookup(params=self.embeddings, ids=self.inputs)
        
        cell_forward = self.build_cells()
        outputs, state = tf.nn.dynamic_rnn(cell=cell_forward,
                                           inputs=self.inputs_embedded,swap_memory=True ,
                                           dtype=self.dtype, time_major=False)
                                           
        attention_r = self.task_specific_attention(outputs)
        output_layer = Dense(self.class_num)
        self.logits = output_layer(attention_r)
        self.cal_loss(self.logits)
       
    def build_model_2att(self):
        print('create model!')
        self.init_placeholders()
        self.inputs_embedded = tf.nn.embedding_lookup(params=self.embeddings, ids=self.inputs)

        cell_forward = self.build_cells()

        outputs, state = tf.nn.dynamic_rnn(cell=cell_forward,
                                   inputs=self.inputs_embedded,swap_memory=True ,
                                   dtype=self.dtype, time_major=False)
        attention_r = self.task_specific_attention(outputs)
        sen_out = tf.cond(self.is_train, lambda: self.task_total_attention(attention_r, "pretrain"), lambda: self.task_total_attention(attention_r, "test"))
        self.attention_raa = sen_out
        self.logits = sen_out
        self.prediction = tf.argmax(tf.nn.softmax(self.logits), axis=1)
        self.total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=sen_out, labels=self.labels))
        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(self.prediction, tf.int32),self.labels), self.dtype))


    def init_optimizer(self):
        print("setting optimizer..")
        # Gradients and SGD update operation for training the model
        trainable_params = tf.trainable_variables()
        if self.optimizer.lower() == 'adadelta':
            self.opt = tf.train.AdadeltaOptimizer(learning_rate=self.learning_rate)
        elif self.optimizer.lower() == 'adam':
            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)
        elif self.optimizer.lower() == 'rmsprop':
            self.opt = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate)
        else:
            self.opt = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)

        gradients = tf.gradients(self.total_loss, trainable_params)

        # Clip gradients by a given maximum_gradient_norm
        clip_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)

        # Update the model
        self.updates = self.opt.apply_gradients(
            zip(clip_gradients, trainable_params), global_step=self.global_step)


    def init_placeholders(self):
        print('init_placeholders')
        self.inputs = tf.placeholder(dtype=tf.int32, shape=(None, None), name='inputs')
        self.labels = tf.placeholder(dtype=tf.int32, shape=(None), name='labels')
        self.query = tf.placeholder(dtype=tf.int32, shape=(None), name='query')
        self.total_shape = tf.placeholder(dtype=tf.int32, shape=(None), name='total_shape')
        self.is_train = tf.placeholder(dtype=tf.bool)
        self.batch_size = tf.shape(self.labels)[0]
        self.total_num = self.total_shape[-1]

    def build_single_cell(self):
        cell_type = LSTMCell
        if (self.cell_type.lower() == 'gru'):
            cell_type = GRUCell
        cell = cell_type(self.hidden_units)

        if self.use_dropout:
            cell = DropoutWrapper(cell, dtype=self.dtype,
                                  output_keep_prob=self.keep_prob_placeholder,)
        if self.use_residual:
            cell = ResidualWrapper(cell)
        return cell

    def build_cells(self):
        return MultiRNNCell([self.build_single_cell() for i in range(self.depth)])

    def check_feeds(self, inputs, label) :
        def init():
           return {},[],[],[],0,[]
        input_feed, seqs, _label, query,  total_num, total_shape = init()
        for bag,_l in zip(inputs, label):
            total_shape.append(total_num)
            total_num += len(bag)
            for seq in bag:
                seqs.append(seq)
                query.append(_l)
            _label.append(_l)
        total_shape.append(total_num)
        input_feed[self.inputs.name] = np.array(seqs)#inputs
        input_feed[self.total_shape.name] = np.array(total_shape)
        input_feed[self.labels.name] = np.array(_label)
        input_feed[self.query.name] = np.array(query)
        return input_feed


    def pretrain(self, sess, inputs, label):
        input_feed = self.check_feeds(inputs, label)
        input_feed[self.is_train.name] = True
        output_feed = [self.total_loss, self.accuracy, self.prediction, self.logits, self.summary_op, self.updates, self.labels, self.attention_raa]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def eval(self, sess, inputs, label):
        input_feed = self.check_feeds(inputs, label)
        input_feed[self.is_train.name] = False
        output_feed = [self.total_loss, self.accuracy, self.prediction, self.logits]
        outputs = sess.run(output_feed, input_feed)
        return outputs
    
    def predict(self, sess, inputs, label):
        input_feed = self.check_feeds(inputs, label)
        input_feed[self.is_train.name] = False
        output_feed = [self.prediction, self.logits, self.accuracy, self.total_loss]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def save(self, sess, path, var_list=None, global_step=None):
        # var_list = None returns the list of all saveable variables
        saver = tf.train.Saver(var_list)

        # temporary code
        #del tf.get_collection_ref('LAYER_NAME_UIDS')[0]
        save_path = saver.save(sess, save_path=path, global_step=global_step)
        print('model saved at %s' % save_path)

    def restore(self, sess, save_path):
        ckpt = tf.train.get_checkpoint_state(save_path)
        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
             print('restore model!')
             saver = tf.train.Saver()
             saver.restore(sess, ckpt.model_checkpoint_path)
        else:
            if not os.path.exists(save_path):
                os.makedirs(save_path)
            sess.run(tf.global_variables_initializer()) 
 
def define():
     config = {}
     config['save_path'] = 'model/detector/'
     config['cell_type'] = 'lstm'
     config['depth'] = 2
     config['source_vocabulary'] = 'data/voc.json'
     config['use_fp16'] = False
     config['class_num'] = 2
     config['use_dropout'] = True
     config['input'] = 'data/train.seq'
     config['valid'] = 'data/valid.seq'
     config['batch_size'] = 50##batch_size must be the same!
     config['voc'] = utils.load_voc(config['source_vocabulary'])
     config['hidden_units'] = 128
     config['maxlen']=500
     config['stride']=250
     config['max_batch']=1000
     config['use_residual'] = False
     config['optimizer'] = 'adam'
     config['learning_rate'] = 0.001
     config['max_gradient_norm'] = 1.0
     config['max_epochs'] = 10000
     config['keep_prob_placeholder'] =1.0
     return config

def predict():
     config = define()

     from data.data_iterator import TextIterator     
     valid_set = TextIterator(source=config['valid'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'])

     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
       model = Detector(config, 'test')
       model.restore(sess, config['save_path'])
       _acc = 0
       _loss = 0
       _num = 0
       for idx, sources in enumerate(valid_set):
              source_seq = sources[0]
              label = sources[1]
              sources, labels = prepare_batch(source_seq, label,max_batch=config['max_batch'], maxlen=config['maxlen'],stride = config['stride'],batch_size=config['batch_size'])
              for source,label in zip(sources, labels):
                 pred, logit, acc, loss= model.predict(sess, source, label)
                 print("step {}, size {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), pred.shape, acc, loss))
                 _acc += acc * pred.shape[0]
                 _loss += loss * pred.shape[0]
                 _num += pred.shape[0]
       print("step {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), _acc/_num, _loss))
       
def train():
     config = define()

     from data.data_iterator import TextIterator     
     test_set = TextIterator(source=config['input'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'],shuffle_each_epoch=True)
     valid_set = TextIterator(source=config['valid'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'],shuffle_each_epoch=False)

     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
       model = Detector(config, 'pretrain')
       model.restore(sess, config['save_path'])
       for epoch_idx in range(config['max_epochs']):
         for idx, sources in enumerate(test_set):
              source_seq = sources[0]
              label = sources[1]
              #source, label, source_len = prepare_batch(source_seq, label, maxlen=config['maxlen'],stride = config['maxlen'],batch_size=config['batch_size'])
              sources, labels = prepare_batch(source_seq, label,max_batch=config['max_batch'], maxlen=config['maxlen'],stride = config['stride'],batch_size=config['batch_size'])
              for source,label in zip(sources, labels):
                 loss, acc, pred, logit, summary, _, _labels, _1= model.pretrain(sess, source, label)
                 print("step {}, size {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), pred.shape, acc, loss))
                 if (model.global_step.eval() % 100 == 0):
                     checkpoint_path = os.path.join(config['save_path'], 'detector')
                     model.save(sess, checkpoint_path, global_step=model.global_step)
                     json.dump(model.config,open('%s-%d.json' % (checkpoint_path, model.global_step.eval()), 'w'),indent=2)
                 if (model.global_step.eval() % 50 == 0):
                     _acc = 0
                     _loss = 0
                     _num = 0
                     for idx, sources in enumerate(valid_set):
                            source_seq = sources[0]
                            label = sources[1]
                            sources, labels = prepare_batch(source_seq, label,max_batch=config['max_batch'], maxlen=config['maxlen'],stride = config['stride'],batch_size=config['batch_size'])
                            for source,label in zip(sources, labels):
                               pred, logit, acc, loss= model.predict(sess, source, label)
                               print("step {}, size {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), pred.shape, acc, loss))
                               _acc += acc * pred.shape[0]
                               _loss += loss * pred.shape[0]
                               _num += pred.shape[0]
                     print("step {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), _acc/_num, _loss))


if __name__ == '__main__':
     train()
     #predict()
