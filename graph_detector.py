import os
import numpy as np
import tensorflow as tf
from utils import utils

save_path = 'detector/gatt-label-size8/'
batch_size =  8
nb_epochs = 100000
patience = 100
lr = 0.005  # learning rate
l2_coef = 0.0005  # weight decay
hid_units = [8] # numbers of hidden units per each attention head in each layer
n_heads = [8, 1] # additional entry for the output layer
residual = False
nonlinearity = tf.nn.elu


class Detector(object):
    def __init__(self, mode, nb_nodes=284, ft_size=284, nb_classes=2):
      with tf.variable_scope('detector',reuse=tf.AUTO_REUSE):
        self.global_step = tf.Variable(0, trainable=False, name='global_step')
        self.mode = mode.lower()
        self.nb_nodes = nb_nodes
        self.ft_size = ft_size
        self.nb_classes = nb_classes
        self.build_model()

        if self.mode == 'pretrain':
            self.init_optimizer()
        elif self.mode == 'test':
            pass

    def init_optimizer(self):
        print("setting optimizer..")
        vars = tf.trainable_variables()
        opt = tf.train.AdamOptimizer(learning_rate=lr)
        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not
                           in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef
        self.train_op = opt.minimize(self.loss+lossL2,global_step=self.global_step)

    def gat(self, inputs, bias_mat, activation=tf.nn.elu, residual=False):
        attns = []
        for _ in range(n_heads[0]):
            attns.append(self.attn_head(inputs, bias_mat=bias_mat,
                out_sz=hid_units[0], activation=activation,
                in_drop=self.ffd_drop, coef_drop=self.attn_drop, residual=False))
        h_1 = tf.concat(attns, axis=-1)
        for i in range(1, len(hid_units)):
            h_old = h_1
            attns = []
            for _ in range(n_heads[i]):
                attns.append(self.attn_head(h_1, bias_mat=bias_mat,
                    out_sz=hid_units[i], activation=activation,
                    in_drop=self.ffd_drop, coef_drop=self.attn_drop, residual=residual))
            h_1 = tf.concat(attns, axis=-1)
        out = []
        for i in range(n_heads[-1]):
            out.append(self.attn_head(h_1, bias_mat=bias_mat,
                out_sz=self.nb_classes, activation=lambda x: x,
                in_drop=self.ffd_drop, coef_drop=self.attn_drop, residual=False))
        logits = tf.add_n(out) / n_heads[-1]
        #return tf.reduce_sum(tf.reshape(logits, [self.nb_nodes, self.nb_classes]), axis=0)
        return tf.reshape(logits, [self.nb_nodes, self.nb_classes])

    def attn_head(self, seq, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False):
      with tf.name_scope('my_attn'):
        if in_drop != 0.0:
            seq = tf.nn.dropout(seq, 1.0 - in_drop)

        seq_fts = tf.layers.conv1d(seq, out_sz, 1, use_bias=False)
        # simplest self-attention possible
        f_1 = tf.layers.conv1d(seq_fts, 1, 1)
        f_2 = tf.layers.conv1d(seq_fts, 1, 1)
        # simplest self-attention possible
        logits = f_1 + tf.transpose(f_2, [0, 2, 1])
        coefs = tf.nn.softmax(tf.matmul(tf.squeeze(tf.nn.leaky_relu(logits)), bias_mat))
        coefs = tf.expand_dims(coefs, 0)
        if coef_drop != 0.0:
            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)
        if in_drop != 0.0:
            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)

        vals = tf.matmul(coefs, seq_fts)
        ret = tf.contrib.layers.bias_add(vals)

        # residual connection
        if residual:
            if seq.shape[-1] != ret.shape[-1]:
                ret = ret + conv1d(seq, ret.shape[-1], 1) # activation
            else:
                ret = ret + seq

        return activation(ret)  # activation


    def inference(self, inputs, bias_mat, activation=tf.nn.elu, residual=False):
        inputs = tf.expand_dims(inputs, 0)#tf.reshape([1, self.nb_nodes, inputs.shape[1]], inputs)
        logits = tf.map_fn(lambda adj: self.gat(inputs, adj, activation, residual), bias_mat)
        logits = self.task_specific_attention(logits, tf.shape(bias_mat)[0])
        return logits

    def task_specific_attention(self, inputs, bsize):
        with tf.variable_scope('specific_attention') as scope:
            attention_context_vector = tf.get_variable(name='attention_context_vector', shape=[self.nb_classes,1],  dtype=tf.float32)
            return tf.reshape(tf.matmul(tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(tf.tanh(inputs), [-1, self.nb_classes]),attention_context_vector),[bsize, -1])),[bsize,1, -1]), inputs),[bsize, self.nb_classes])

    def softmax_cross_entropy(self, logits, labels):
        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)
        return tf.reduce_mean(loss)

    def sigmoid_cross_entropy(self, logits, labels):
        labels = tf.cast(labels, dtype=tf.float32)
        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)
        loss=tf.reduce_mean(loss,axis=1)
        return tf.reduce_mean(loss)

    def cal_accuracy(self, logits, labels):
        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))
        accuracy_all = tf.cast(correct_prediction, tf.float32)
        return tf.reduce_mean(accuracy_all)

    def build_model(self):
        print('create model!')
        self.init_placeholders()
        self.logits = self.inference(self.ftr_in,bias_mat=self.bias_in,
                                residual=residual, activation=nonlinearity)
        log_resh = tf.reshape(self.logits, [-1, self.nb_classes])
        lab_resh = tf.reshape(self.lbl_in, [-1, self.nb_classes])
        self.loss = self.softmax_cross_entropy(log_resh, lab_resh) 
        self.accuracy = self.cal_accuracy(log_resh, lab_resh)

    def init_placeholders(self):
        print('init_placeholders')
        with tf.name_scope('input'):
            self.ftr_in = tf.placeholder(dtype=tf.float32, shape=(self.nb_nodes, self.ft_size))
            self.bias_in = tf.placeholder(dtype=tf.float32, shape=(None, self.nb_nodes, self.nb_nodes))
            self.lbl_in = tf.placeholder(dtype=tf.int32, shape=(None, self.nb_classes))
            self.attn_drop = tf.placeholder(dtype=tf.float32, shape=())
            self.ffd_drop = tf.placeholder(dtype=tf.float32, shape=())
            self.is_train = tf.placeholder(dtype=tf.bool, shape=())

    def check_feeds(self, features, graph, lbl, attn, ffd, is_train) :
        input_feed={}
        input_feed[self.ftr_in] = np.array(features)
        input_feed[self.bias_in] = np.array(graph)
        input_feed[self.lbl_in] = np.array(lbl)
        input_feed[self.attn_drop] = attn
        input_feed[self.ffd_drop] = ffd
        input_feed[self.is_train] = is_train
        return input_feed

    def pretrain(self, sess, features, graph, lbl, attn=0.6, ffd=0.6):
        input_feed = self.check_feeds(features, graph, lbl, attn, ffd, True)
        output_feed = [self.train_op, self.loss, self.accuracy]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def predict(self, sess, features, graph, lbl, attn=0.6, ffd=0.6):
        input_feed = self.check_feeds(features, graph, lbl, attn, ffd, False)
        output_feed = [self.loss, self.accuracy]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def eval(self, sess, features, graph, lbl, attn=0.6, ffd=0.6):
        input_feed = self.check_feeds(features, graph, lbl, attn, ffd, False)
        output_feed = [self.loss, self.accuracy]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def save(self, sess, path, var_list=None, global_step=None):
        # var_list = None returns the list of all saveable variables
        saver = tf.train.Saver(var_list)

        # temporary code
        #del tf.get_collection_ref('LAYER_NAME_UIDS')[0]
        save_path = saver.save(sess, save_path=path, global_step=global_step)
        print('model saved at %s' % save_path)

    def restore(self, sess, save_path):
        ckpt = tf.train.get_checkpoint_state(save_path)
        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
             print('restore model!')
             saver = tf.train.Saver()
             saver.restore(sess, ckpt.model_checkpoint_path)
        else:
            if not os.path.exists(save_path):
                os.makedirs(save_path)
            sess.run(tf.global_variables_initializer())

    def restore_specific(self, sess, save_path):
        saver = tf.train.Saver()
        saver.restore(sess, save_path)


def read_files():
    features, labels, valid_adj, valid_graph_labels, train_adj, train_graph_labels = utils.load_data()

    nb_nodes = features.shape[0]
    ft_size = features.shape[1]
    nb_classes = train_graph_labels.shape[1]
    return features, labels, valid_adj, valid_graph_labels, train_adj, train_graph_labels, nb_nodes,ft_size,nb_classes

def train():
    tf.reset_default_graph()
    features, labels, valid_adj, valid_graph_labels, train_adj, train_graph_labels, nb_nodes,ft_size,nb_classes = read_files()
    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
        model = Detector('pretrain')
        model.restore(sess, save_path)
        vlss_mn = np.inf
        vacc_mx = 0.90
        curr_step = 0

        for epoch in range(nb_epochs):
            tr_step = 0
            tr_size = train_adj.shape[0]
            while tr_step < tr_size:
                _, tr_loss_value_tr, tr_acc_tr = model.pretrain(sess, features, train_adj[tr_step:tr_step+batch_size],  train_graph_labels[tr_step:tr_step+batch_size])
                tr_step += batch_size
                if (model.global_step.eval() % 1000 == 0):
                  vl_step = 0
                  vl_times = 0
                  val_acc_avg = 0
                  val_loss_avg = 0
                  vl_size = valid_adj.shape[0]
                  while vl_step < vl_size:
                     loss_value_vl, acc_vl = model.eval(sess, features, valid_adj[vl_step:vl_step+batch_size],valid_graph_labels[vl_step:vl_step+batch_size])
                     val_loss_avg += loss_value_vl
                     val_acc_avg += acc_vl
                     vl_step += batch_size
                     vl_times += 1
                  val_loss_avg = val_loss_avg/vl_times
                  val_acc_avg = val_acc_avg/vl_times
                  print('epoch: %d, global_step: %d, Training: tr_size = %d, loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %
                    (epoch, model.global_step.eval(), tr_step, tr_loss_value_tr, tr_acc_tr,
                    val_loss_avg, val_acc_avg))
                  if val_acc_avg >= vacc_mx:
                     model.save(sess, save_path, global_step=model.global_step)
                     vacc_mx = val_acc_avg

train()
