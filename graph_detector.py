import os
import numpy as np
import tensorflow as tf
from utils import utils
from sklearn.metrics import roc_curve, auc, accuracy_score, recall_score, precision_score

save_path = 'model/graph_detector/gatt-label-size8/'
batch_size =  200
nb_epochs = 200
patience = 1000
lr = 0.005  # learning rate
l2_coef = 0.0005  # weight decay
hid_units = [8] # numbers of hidden units per each attention head in each layer
n_heads = [8, 1] # additional entry for the output layer
residual = False
nonlinearity = tf.nn.elu
max_gradient_norm = 1.0
withmatmul = True
withattention = True
acc = 0.00

class Detector(object):
    def __init__(self, mode, nb_nodes=284, ft_size=284, nb_classes=2):
      with tf.variable_scope('graph_detector',reuse=tf.AUTO_REUSE):
        self.global_step = tf.Variable(0, trainable=False, name='global_step')
        self.mode = mode.lower()
        self.nb_nodes = nb_nodes
        self.ft_size = ft_size
        self.nb_classes = nb_classes
        
        self.init_placeholders()
        self.build_model()
        if self.mode == 'pretrain':
            self.init_optimizer()
        elif self.mode == 'test':
            pass

    def init_optimizer(self):
        print("setting optimizer..")
        #trainable_params = tf.trainable_variables()
        #print(trainable_params)
        opt = tf.train.AdamOptimizer(learning_rate=lr)
        #gradients = tf.gradients(self.loss, trainable_params)
        #clip_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)
        #self.train_op = opt.apply_gradients(
        #    zip(clip_gradients, trainable_params), global_step=self.global_step)
        vars = tf.trainable_variables()
        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not
                           in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef
        self.train_op = opt.minimize(self.loss+lossL2,global_step=self.global_step)

    def gat(self, inputs, bias_mat, activation=tf.nn.elu, residual=False):
        attns = []
        for _ in range(n_heads[0]):
            attns.append(self.attn_head(inputs, bias_mat=bias_mat,
                out_sz=hid_units[0], activation=activation,
                in_drop=self.ffd_drop, coef_drop=self.attn_drop, residual=False))
        h_1 = tf.concat(attns, axis=-1)
        for i in range(1, len(hid_units)):
            h_old = h_1
            attns = []
            for _ in range(n_heads[i]):
                attns.append(self.attn_head(h_1, bias_mat=bias_mat,
                    out_sz=hid_units[i], activation=activation,
                    in_drop=self.ffd_drop, coef_drop=self.attn_drop, residual=residual))
            h_1 = tf.concat(attns, axis=-1)
        out = []
        for i in range(n_heads[-1]):
            out.append(self.attn_head(h_1, bias_mat=bias_mat,
                out_sz=self.nb_classes, activation=lambda x: x,
                in_drop=self.ffd_drop, coef_drop=self.attn_drop, residual=False))
        logits = tf.add_n(out) / n_heads[-1]
        return tf.reshape(logits, [self.nb_nodes, self.nb_classes])

    def attn_head(self, seq, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False):
      with tf.name_scope('my_attn') as scope:
        if in_drop != 0.0:
            seq = tf.nn.dropout(seq, 1.0 - in_drop)

        seq_fts = tf.layers.conv1d(seq, out_sz, 1, use_bias=False)
        # simplest self-attention possible
        f_1 = tf.layers.conv1d(seq_fts, 1, 1)
        f_2 = tf.layers.conv1d(seq_fts, 1, 1)
        # simplest self-attention possible
        logits = f_1 + tf.transpose(f_2, [0, 2, 1])
        if withmatmul:
            coefs = tf.nn.softmax(tf.matmul(tf.squeeze(tf.nn.leaky_relu(logits)), bias_mat))
        else:
            coefs = tf.nn.softmax(tf.nn.leaky_relu(logits)+bias_mat)
        coefs = tf.expand_dims(coefs, 0)
        if coef_drop != 0.0:
            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)
        if in_drop != 0.0:
            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)

        vals = tf.matmul(coefs, seq_fts)
        ret = tf.contrib.layers.bias_add(vals)

        # residual connection
        if residual:
            if seq.shape[-1] != ret.shape[-1]:
                ret = ret + conv1d(seq, ret.shape[-1], 1) # activation
            else:
                ret = ret + seq

        return activation(ret)  # activation


    def inference(self, inputs, bias_mat):
        inputs = tf.expand_dims(inputs, 0)#tf.reshape([1, self.nb_nodes, inputs.shape[1]], inputs)
        logits = tf.map_fn(lambda adj: self.gat(inputs, adj, nonlinearity, residual), bias_mat)
        if withattention:
           logits = self.task_specific_attention(logits, tf.shape(bias_mat)[0])
        else:
           logits =  tf.reduce_sum(logits, axis=1)
        return logits

    def task_specific_attention(self, inputs, bsize):
        with tf.variable_scope('specific_attention') as scope:
            attention_context_vector = tf.get_variable(name='attention_context_vector', shape=[self.nb_classes,1],  dtype=tf.float32)
            return tf.reshape(tf.matmul(tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(tf.tanh(inputs), [-1, self.nb_classes]),attention_context_vector),[bsize, -1])),[bsize,1, -1]), inputs),[bsize, self.nb_classes])

    def softmax_cross_entropy(self, logits, labels):
        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tf.argmax(labels,1))
        return tf.reduce_mean(loss)

    def sigmoid_cross_entropy(self, logits, labels):
        labels = tf.cast(labels, dtype=tf.float32)
        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)
        loss=tf.reduce_mean(loss,axis=1)
        return tf.reduce_mean(loss)

    def cal_accuracy(self, logits, labels):
        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))
        accuracy_all = tf.cast(correct_prediction, tf.float32)
        return tf.reduce_mean(accuracy_all, name='accuracy')

    def build_model(self):
        print('build model!')
        self.logits = self.inference(self.ftr_in,bias_mat=self.bias_in)
        log_resh = tf.reshape(self.logits, [-1, self.nb_classes])
        lab_resh = tf.reshape(self.lbl_in, [-1, self.nb_classes])
        self.logit = log_resh
        self.pred = tf.argmax(log_resh, 1)
        self.loss = self.softmax_cross_entropy(log_resh, lab_resh) 
        self.accuracy = self.cal_accuracy(log_resh, lab_resh)

    def init_placeholders(self):
        print('init_placeholders')
        with tf.name_scope('input'):
            self.ftr_in = tf.placeholder(dtype=tf.float32, shape=(self.nb_nodes, self.ft_size), name="ftr_in")
            self.bias_in = tf.placeholder(dtype=tf.float32, shape=(None, self.nb_nodes, self.nb_nodes), name='bias_in')
            self.lbl_in = tf.placeholder(dtype=tf.int32, shape=(None, self.nb_classes), name='lbl_in')
            self.attn_drop = tf.placeholder(dtype=tf.float32, shape=(), name='attn_drop')
            self.ffd_drop = tf.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')
            self.is_train = tf.placeholder(dtype=tf.bool, shape=(), name='is_train')

    def check_feeds(self, features, graph, lbl, attn, ffd, is_train) :
        input_feed={}
        input_feed[self.ftr_in.name] = np.array(features)
        input_feed[self.bias_in.name] = np.array(graph)
        input_feed[self.lbl_in.name] = np.array(lbl)
        input_feed[self.attn_drop.name] = attn
        input_feed[self.ffd_drop.name] = ffd
        input_feed[self.is_train.name] = is_train
        return input_feed

    def pretrain(self, sess, features, graph, lbl, attn=0.5, ffd=0.5):
        input_feed = self.check_feeds(features, graph, lbl, attn, ffd, True)
        output_feed = [self.train_op, self.loss, self.accuracy]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def predict(self, sess, features, graph, lbl, attn=0, ffd=0):
        input_feed = self.check_feeds(features, graph, lbl, attn, ffd, False)
        output_feed = [self.loss, self.accuracy, self.pred, self.logit]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def eval(self, sess, features, graph, lbl, attn=0, ffd=0):
        input_feed = self.check_feeds(features, graph, lbl, attn, ffd, False)
        output_feed = [self.loss, self.accuracy, self.pred, self.logit]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def checkWeight(self,sess):
        for tv in tf.trainable_variables():
           print(tv.name +':\n')
           print( sess.run([tf.get_default_graph().get_tensor_by_name(tv.name)]))

    def save(self, sess, path, var_list=None, global_step=None):
        # var_list = None returns the list of all saveable variables
        saver = tf.train.Saver()

        # temporary code
        #del tf.get_collection_ref('LAYER_NAME_UIDS')[0]
        save_path = saver.save(sess, save_path=path, global_step=global_step)
        print('model saved at %s' % save_path)

    def restore(self, sess, save_path):
        ckpt = tf.train.get_checkpoint_state(save_path)
        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
             print('restore model!'+ ckpt.model_checkpoint_path)
             #from tensorflow.python import pywrap_tensorflow
             #reader = pywrap_tensorflow.NewCheckpointReader(ckpt.model_checkpoint_path)
             #var_to_shape_map = reader.get_variable_to_shape_map()
             #for key in var_to_shape_map:
             #   print("tensor_name: ", key)
             saver = tf.train.Saver()
             saver.restore(sess, ckpt.model_checkpoint_path)
        else:
            if not os.path.exists(save_path):
                os.makedirs(save_path)
            sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))

    def restore_specific(self, sess, save_path):
        saver = tf.train.Saver()
        saver.restore(sess, save_path)


def read_files():
    features, labels, valid_adj, valid_graph_labels, train_adj, train_graph_labels = utils.load_data()

    nb_nodes = features.shape[0]
    ft_size = features.shape[1]
    nb_classes = train_graph_labels.shape[1]
    return features, labels, valid_adj, valid_graph_labels, train_adj, train_graph_labels, nb_nodes,ft_size,nb_classes


def predict():
    tf.reset_default_graph()
    features, labels, valid_adj, valid_graph_labels, train_adj, train_graph_labels, nb_nodes,ft_size,nb_classes = read_files()
    with tf.Graph().as_default():
     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
       model = Detector('test')
       model.restore(sess,save_path)
       val_loss_avg = 0
       val_acc_avg = 0
       vl_step = 0
       vl_size = valid_adj.shape[0]
       prediction = []
       all_labels = []
       all_pred = []
       while vl_step < vl_size:
           loss_value_vl, acc_vl, pred, logit = model.eval(sess, features, valid_adj[vl_step:vl_step+batch_size],valid_graph_labels[vl_step:vl_step+batch_size])
           prediction.extend(logit)
           all_pred.extend(list(map(int,pred)))
           all_labels.extend(valid_graph_labels[vl_step:vl_step+batch_size])
           val_loss_avg += loss_value_vl * pred.shape[0]
           val_acc_avg += acc_vl * pred.shape[0]
           vl_step += pred.shape[0]

       val_loss_avg = val_loss_avg/vl_step
       val_acc_avg = val_acc_avg/vl_step
       print('global_step: %d, Val: loss = %.5f, acc = %.5f' % (model.global_step.eval(), val_loss_avg, val_acc_avg))
       prediction = np.stack(prediction)
       all_labels = np.stack(all_labels)
       all_pred = np.stack(all_pred)
    return prediction, all_labels, [accuracy_score(np.argmax(all_labels, 1), all_pred), precision_score(np.argmax(all_labels, 1), all_pred), recall_score(np.argmax(all_labels, 1), all_pred)]

def train():
    tf.reset_default_graph()
    features, labels, valid_adj, valid_graph_labels, train_adj, train_graph_labels, nb_nodes,ft_size,nb_classes = read_files()
    with tf.Graph().as_default():
      with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
        model = Detector('pretrain')
        model.restore(sess, save_path)
        wrong = 0
        vacc_mx = acc
        for epoch in range(nb_epochs):
            tr_step = 0
            tr_size = train_adj.shape[0]
            while tr_step < tr_size:
                if wrong > patience:
                   return
                _, tr_loss_value_tr, tr_acc_tr = model.pretrain(sess, features, train_adj[tr_step:tr_step+batch_size],  train_graph_labels[tr_step:tr_step+batch_size])
                tr_step += batch_size
                if (model.global_step.eval() % 100 == 0):
                  vl_step = 0
                  val_acc_avg = 0
                  val_loss_avg = 0
                  vl_size = valid_adj.shape[0]
                  while vl_step < vl_size:
                     loss_value_vl, acc_vl, pred, logit = model.eval(sess, features, valid_adj[vl_step:vl_step+batch_size],valid_graph_labels[vl_step:vl_step+batch_size])
                     val_loss_avg += loss_value_vl * pred.shape[0]
                     val_acc_avg += acc_vl * pred.shape[0]
                     vl_step += pred.shape[0]
                  val_loss_avg = val_loss_avg/vl_step
                  val_acc_avg = val_acc_avg/vl_step
                  print('epoch: %d, global_step: %d, wrong:%d, Training: tr_size = %d, loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' % (epoch, model.global_step.eval(), wrong, tr_size, tr_loss_value_tr, tr_acc_tr, val_loss_avg, val_acc_avg))
                  if val_acc_avg >= vacc_mx:
                     model.save(sess, save_path, global_step=model.global_step)
                     vacc_mx = val_acc_avg
                     wrong = 0
                  else:
                     wrong += 1
             

#predict()
#train()
#predict()
