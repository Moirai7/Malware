from detector import *
import datetime
from data.data_utils import prepare_batch_without_window

def predict_without_window(config):
     config['max_epochs']=15
     config['batch_size']=6
     tf.reset_default_graph()
     from data.data_iterator import TextIterator
     valid_set = TextIterator(source=config['valid'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'],
                            shuffle_each_epoch=False)
     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
       model = Detector(config, 'test')
       model.restore(sess, config['save_path'])
       _acc = 0
       _loss = 0
       _num = 0
       prediction = []
       all_labels = []
       all_pred = []
       for idx, test_sources in enumerate(valid_set):
             sub_source_seq = test_sources[0]
             sub_label = test_sources[1]
             sub_sources, _ = prepare_batch_without_window(sub_source_seq, batch_size=config['batch_size'])
             pred, logit, acc, loss= model.predict(sess, sub_sources, sub_label)
             print("step {}, size {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), pred.shape, acc, loss))
             prediction.extend(logit)
             all_labels.extend(list(map(int,sub_label)))
             all_pred.extend(list(map(int,pred)))
             _acc += acc * pred.shape[0]
             _loss += loss * pred.shape[0]
             _num += pred.shape[0]
       print("step {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), _acc/_num, _loss))
       prediction = np.stack(prediction)
       all_labels = np.stack(all_labels)
       all_pred = np.stack(all_pred)
       return prediction, label_binarize(all_labels,classes=[0,1,2]), accuracy_score(all_labels, all_pred)

def train_without_window(config, maxs):
     config['max_epochs']=15
     config['batch_size']=6
     tf.reset_default_graph()
     from data.data_iterator import TextIterator
     test_set = TextIterator(source=config['input'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'],
                            shuffle_each_epoch=True)
     valid_set = TextIterator(source=config['valid'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'],
                            shuffle_each_epoch=False)
     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
       model = Detector(config, 'pretrain')
       model.restore(sess, config['save_path'])
       for epoch_idx in range(config['max_epochs']):
         print(epoch_idx)
         oldtime=datetime.datetime.now()
         for idx, train_sources  in enumerate(test_set):
              source_seq = train_sources[0]
              label = train_sources[1]
              sources, _ = prepare_batch_without_window(source_seq, batch_size=config['batch_size'])
              loss, acc, pred, logit, summary, _, _labels, _1= model.pretrain(sess, sources, label)
              print("step {}, size {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), pred.shape, acc, loss))
         newtime=datetime.datetime.now()
         print('%s microseconds'%(newtime-oldtime).seconds)
         _acc = 0
         _loss = 0
         _num = 0
         for idx, test_sources in enumerate(valid_set):
             sub_source_seq = test_sources[0]
             sub_label = test_sources[1]
             sub_sources, _ = prepare_batch_without_window(sub_source_seq, batch_size=config['batch_size'])
             pred, logit, acc, loss= model.predict(sess, sub_sources, sub_label)
             print("step {}, size {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), pred.shape, acc, loss))
             _acc += acc * pred.shape[0]
             _loss += loss * pred.shape[0]
             _num += pred.shape[0]
         print("step {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), _acc/_num, _loss))
         if _acc/_num > maxs:
             checkpoint_path = os.path.join(config['save_path'], 'detector')
             model.save(sess, checkpoint_path, global_step=model.global_step)
             json.dump(model.config,open('%s-%d.json' % (checkpoint_path, model.global_step.eval()), 'w'),indent=2)
             maxs = _acc/_num


res = []
strs = config['save_path'].strip('/')
maxs = 0.5         
config['save_path'] = strs+'_winszero/'
pred,label,acc = predict_without_window(config)
print(acc)
fpr,tpr,roc_auc = cal_roc(pred,label)
label = 'windows length: zero'
res.append([fpr,tpr,roc_auc,label])
#train_without_window(config, maxs)
checked = list(range(100,1000,100))
for wins in range(100, 1000, 200):
    config['maxlen'] = wins
    config['stride'] = int(wins*0.5)
    config['save_path'] = strs+'_wins'+str(config['maxlen'])+'/'
    maxs = 0.5
    if wins not in checked:
        oldtime=datetime.datetime.now()
        train(config, maxs)
        newtime=datetime.datetime.now()
        print('%s microseconds'%(newtime-oldtime).seconds)
        continue
    pred,label,acc = predict(config)
    print(acc)
    fpr,tpr,roc_auc = cal_roc(pred,label)
    label = 'windows length: %d' % wins
    res.append([fpr,tpr,roc_auc,label])

savefig(res,0)
savefig(res,1)
savefig(res,'micro')
