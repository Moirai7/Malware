
#!/usr/bin/env python
# coding: utf-8

import os
import math
import time
import json
import random

from collections import OrderedDict

import numpy as np
import tensorflow as tf

from data.data_iterator import TextIterator

import data.util as util
import data.data_utils as data_utils
from data.data_utils import prepare_batch
from data.data_utils import prepare_train_batch

from seq2seq_model import Seq2SeqModel

tf.app.flags.DEFINE_string('source_train_data', 'data/train.seq', 'Path to source training data')
tf.app.flags.DEFINE_string('source_valid_data', 'data/valid.seq', 'Path to source validation data')
tf.app.flags.DEFINE_string('model_path', 'model/', 'Path to model checkpoint dir.')

# Decoding parameters
tf.app.flags.DEFINE_integer('beam_width', 4, 'Beam width used in beamsearch')
tf.app.flags.DEFINE_integer('decode_batch_size', 2, 'Batch size used for decoding')
tf.app.flags.DEFINE_integer('max_decode_step', 10, 'Maximum time step limit to decode')
tf.app.flags.DEFINE_boolean('write_n_best', False, 'Write n-best list (n=beam_width)')

# Runtime parameters
tf.app.flags.DEFINE_boolean('allow_soft_placement', True, 'Allow device soft placement')
tf.app.flags.DEFINE_boolean('log_device_placement', False, 'Log placement of ops on devices')

# Training parameteres
tf.app.flags.DEFINE_integer('max_epochs', 11110, 'Maximum # of training epochs')
tf.app.flags.DEFINE_integer('display_freq', 1, 'Display training status every this iteration')
tf.app.flags.DEFINE_integer('save_freq', 100, 'Save model checkpoint every this iteration')
tf.app.flags.DEFINE_integer('valid_freq', 10, 'Evaluate model every this iteration: valid_data needed')
tf.app.flags.DEFINE_string('optimizer', 'adam', 'Optimizer for training: (adadelta, adam, rmsprop)')
tf.app.flags.DEFINE_string('model_name', 'translate.ckpt', 'File name used for model checkpoints')
FLAGS = tf.app.flags.FLAGS

def load_config(FLAGS):
    if os.path.isdir(FLAGS.model_path):
        ckpt = tf.train.get_checkpoint_state(FLAGS.model_path)
        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
              FLAGS.model_path = ckpt.model_checkpoint_path
    config = util.load_dict('%s.json' % FLAGS.model_path)
    for key, value in FLAGS.__flags.items():
        config[key] = value

    return config


def load_model(session, config):
    model = Seq2SeqModel(config, 'train')
    if tf.train.checkpoint_exists(FLAGS.model_path):
        print('Reloading model parameters..')
        model.restore(session, FLAGS.model_path)
    else:
        raise ValueError(
            'No such file:[{}]'.format(FLAGS.model_path))
    return model


def train():
    # Load model config
    config = load_config(FLAGS)

    # Load source data to decode
    test_set = TextIterator(source=config['source_train_data'],
                            batch_size=config['decode_batch_size'],
                            source_dict=config['source_vocabulary'],
                            maxlen=None,
                            n_words_source=config['num_encoder_symbols'])
    #test_set, test_labels = data_utils.load_data('test')
    #valid_set, valid_labels = data_utils.load_data('valid')
    # Load inverse dictionary used in decoding
    
    # Initiate TF session
    with tf.Session(config=tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement, 
        log_device_placement=FLAGS.log_device_placement, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:

        # Reload existing checkpoint
        model = load_model(sess, config)

        # Create a log writer object
        log_writer = tf.summary.FileWriter(FLAGS.model_path, graph=sess.graph)

        step_time, loss = 0.0, 0.0
        words_seen, sents_seen = 0, 0
        start_time = time.time()

        for epoch_idx in range(FLAGS.max_epochs):
            if model.global_epoch_step.eval() >= FLAGS.max_epochs:
                print('Training is already complete.', \
                      'current epoch:{}, max epoch:{}'.format(model.global_epoch_step.eval(), FLAGS.max_epochs))
                break
            for idx, source_seq in enumerate(test_set):
                source, source_len = prepare_batch(source_seq)
                # predicted_ids: GreedyDecoder; [batch_size, max_time_step, 1]
                # BeamSearchDecoder; [batch_size, max_time_step, beam_width]
                step_loss, summary, predicted_ids = model.train(sess, encoder_inputs=source, 
                                              encoder_inputs_length=source_len)
                loss += float(step_loss) / FLAGS.display_freq
                words_seen += float(np.sum(source_len+target_len))
                sents_seen += float(source.shape[0]) # batch_size
                if model.global_step.eval() % FLAGS.display_freq == 0:

                    avg_perplexity = math.exp(float(loss)) if loss < 300 else float("inf")

                    time_elapsed = time.time() - start_time
                    step_time = time_elapsed / FLAGS.display_freq

                    words_per_sec = words_seen / time_elapsed
                    sents_per_sec = sents_seen / time_elapsed

                    print('Epoch ', model.global_epoch_step.eval(), 'Step ', model.global_step.eval(), \
                          'Perplexity {0:.2f}'.format(avg_perplexity), 'Step-time ', step_time, \
                          '{0:.2f} sents/s'.format(sents_per_sec), '{0:.2f} words/s'.format(words_per_sec))

                    loss = 0
                    words_seen = 0
                    sents_seen = 0
                    start_time = time.time()

                    # Record training summary for the current batch
                    log_writer.add_summary(summary, model.global_step.eval())

                    # Write decoding results
                    print(str(source_seq),'\t',)
                    for i in range(len(source_seq)):
                        res = []
                        for seq in predicted_ids:
                            res.append(list(seq[:,i]))
                        print(str(res))
                    print('  {}th line decoded'.format(idx * FLAGS.decode_batch_size))
                if valid_set and model.global_step.eval() % FLAGS.valid_freq == 0:
                    print('Validation step')
                    valid_loss = 0.0
                    valid_sents_seen = 0                
                    for idx, source_seq in enumerate(valid_set):
                        label = test_labels[idx]
                        source, source_len = prepare_batch(source_seq)
                        step_loss, summary, predicted_ids = model.train(sess, encoder_inputs=source,
                                              encoder_inputs_length=source_len)
                        batch_size = source.shape[0]
                        valid_loss += step_loss * batch_size
                        valid_sents_seen += batch_size
                    valid_loss = valid_loss / valid_sents_seen
                    print('Valid perplexity: {0:.2f}'.format(math.exp(valid_loss)))

                # Save the model checkpoint
                if model.global_step.eval() % FLAGS.save_freq == 0:
                    print('Saving the model..')
                    checkpoint_path = os.path.join(FLAGS.model_dir, FLAGS.model_name)
                    model.save(sess, checkpoint_path, global_step=model.global_step)
                    json.dump(model.config,
                              open('%s-%d.json' % (checkpoint_path, model.global_step.eval()), 'wb'),
                              indent=2)

def main(_):
    train()


if __name__ == '__main__':
    tf.app.run()

