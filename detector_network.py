import math
import os
import numpy as np
import tensorflow as tf
from tensorflow.python.ops.rnn_cell import GRUCell
from tensorflow.python.ops.rnn_cell import LSTMCell
from tensorflow.python.ops.rnn_cell import MultiRNNCell
from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper
from attention import attention
import tensorflow.contrib.layers as layers
from tensorflow.python.layers.core import Dense
from tensorflow.python.framework import dtypes
import data.util as utils
from data.data_utils import prepare_batch
import json

class Detector(object):
    def __init__(self, config, mode):
        self.config = config
        self.mode = mode.lower()
        self.cell_type = config['cell_type']
        self.depth = config['depth']
        self.class_num = config['class_num']
        self.dtype = tf.float16 if config['use_fp16'] else tf.float32
        self.voc = config['voc']
        self.use_residual = config['use_residual']
        self.embeddings = tf.get_variable(initializer=tf.one_hot(self.voc, len(self.voc), dtype=self.dtype), name='emmbedding', trainable=False)
        self.use_dropout = config['use_dropout']
        self.batch_size = config['batch_size']
        self.hidden_units = len(config['voc'])#config['hidden_units']
        self.optimizer = config['optimizer']
        self.learning_rate = config['learning_rate']
        self.max_gradient_norm = config['max_gradient_norm']
        self.big_num = config['big_num']
        self.global_step = tf.Variable(0, trainable=False, name='global_step')
        self.build_model()

    def task_specific_attention(self, inputs, scope=None):
        with tf.variable_scope(scope or 'attention') as scope:
            attention_context_vector = tf.get_variable(name='attention_context_vector', shape=[self.hidden_units,1],  dtype=self.dtype)
            return tf.reshape(tf.matmul(tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(tf.tanh(inputs), [-1, self.hidden_units]),attention_context_vector),[self.batch_size, -1])),[self.batch_size,1, -1]), inputs),[self.batch_size, self.hidden_units])

    def task_total_attention(self, attention_r):
        with tf.variable_scope(scope or 'attention') as scope:
            sen_a = tf.get_variable('attention_A', [self.hidden_units])
            sen_r = tf.get_variable('query_r', [self.hidden_units, 1])
            sen_d = tf.get_variable('bias_d', [self.class_num])
            relation_embedding = tf.get_variable('relation_embedding', [self.class_num, self.hidden_units])
            sen_repre = []
            sen_alpha = []
            sen_s = []
            sen_out = []
            self.prob = []
            self.predictions = []
            self.loss = []
            self.accuracy = []
            self.total_loss = 0.0
            for i in range(self.big_num):
               sen_repre.append(tf.tanh(attention_r[self.total_shape[i]:self.total_shape[i + 1]]))
               batch_size = self.total_shape[i + 1] - self.total_shape[i]
               sen_alpha.append(tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.multiply(sen_repre[i], sen_a), sen_r), [batch_size])), [1, batch_size]))
               sen_s.append(tf.reshape(tf.matmul(sen_alpha[i], sen_repre[i]), [self.hidden_units, 1]))
               sen_out.append(tf.add(tf.reshape(tf.matmul(relation_embedding, sen_s[i]), [self.class_num]), sen_d))
               self.prob.append(tf.nn.softmax(sen_out[i]))
               self.predictions.append(tf.argmax(self.prob[i], 0, name="predictions"))
               self.loss.append(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=sen_out[i], labels=self.input_y[i])))
               if i == 0:
                   self.total_loss = self.loss[i]
               else:
                   self.total_loss += self.loss[i]
               self.accuracy.append(tf.reduce_mean(tf.cast(tf.equal(self.predictions[i], tf.argmax(self.input_y[i], 0)), "float"),name="accuracy"))

    def build_model(self):
        print('create model!')
        self.init_placeholders()
        self.inputs_embedded = tf.nn.embedding_lookup(params=self.embeddings, ids=self.inputs)

        cell_forward = self.build_cells()

        outputs, state = tf.nn.dynamic_rnn(cell=cell_forward,
                                   inputs=self.inputs_embedded,
                                   dtype=self.dtype, time_major=False)
        
        attention_r = self.task_specific_attention(outputs)
        self.task_total_attention(attention_r)
        '''
        with tf.variable_scope('classifier'):
              #self.logits = tf.nn.softmax(self.att_outputs)
              self.logits = layers.fully_connected(self.att_outputs, self.class_num) 
              self.prediction = tf.argmax(self.logits, axis=1)
        with tf.variable_scope('pretrain'):
             self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=self.logits)
             self.total_loss = tf.reduce_mean(self.cross_entropy)
             #self.accuracy = tf.reduce_mean(tf.cast(tf.nn.in_top_k(self.logits, self.labels, 1), self.dtype))
             self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.prediction, self.labels), self.dtype))
        '''

        if self.mode == 'pretrain':
             self.init_optimizer()
        elif self.mode == 'test':
             pass
        
        tf.summary.scalar('loss', self.total_loss)
        tf.summary.scalar('accuracy', self.accuracy)
        self.summary_op = tf.summary.merge_all()

    def init_optimizer(self):
        print("setting optimizer..")
        # Gradients and SGD update operation for training the model
        trainable_params = tf.trainable_variables()
        if self.optimizer.lower() == 'adadelta':
            self.opt = tf.train.AdadeltaOptimizer(learning_rate=self.learning_rate)
        elif self.optimizer.lower() == 'adam':
            self.opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate)
        elif self.optimizer.lower() == 'rmsprop':
            self.opt = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate)
        else:
            self.opt = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)

        gradients = tf.gradients(self.total_loss, trainable_params)

        # Clip gradients by a given maximum_gradient_norm
        clip_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)

        # Update the model
        self.updates = self.opt.apply_gradients(
            zip(clip_gradients, trainable_params), global_step=self.global_step)


    def init_placeholders(self):
        self.keep_prob_placeholder = tf.placeholder(self.dtype, shape=[], name='keep_prob')
        self.inputs = tf.placeholder(dtype=tf.int64, shape=(None, None), name='inputs')
        self.labels = tf.placeholder(dtype=tf.int64, shape=(None), name='labels')
        self.total_shape = tf.placeholder(dtype=tf.int32, shape=[self.big_num + 1], name='total_shape')
        self.batch_size = tf.shape(self.inputs)[0]

    def build_single_cell(self):
        cell_type = LSTMCell
        if (self.cell_type.lower() == 'gru'):
            cell_type = GRUCell
        cell = cell_type(self.hidden_units)

        if self.use_dropout:
            cell = DropoutWrapper(cell, dtype=self.dtype,
                                  output_keep_prob=self.keep_prob_placeholder,)
        if self.use_residual:
            cell = ResidualWrapper(cell)
        return cell

    def build_cells(self):
        return MultiRNNCell([self.build_single_cell() for i in range(self.depth)])

    def pretrain(self, sess, inputs, label):
        input_feed = {}
        input_feed[self.inputs.name] = inputs
        input_feed[self.keep_prob_placeholder.name] = 1.0
        input_feed[self.labels.name] = label
        output_feed = [self.total_loss, self.accuracy, self.prediction, self.logits, self.summary_op, self.updates, self.labels, tf.shape(self.inputs_embedded)]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def eval(self, sess, inputs, label):
        input_feed = {}
        input_feed[self.inputs.name] = inputs
        input_feed[self.keep_prob_placeholder.name] = 1.0
        input_feed[self.labels.name] = label
        output_feed = [self.total_loss, self.accuracy, self.prediction, self.logits]
        outputs = sess.run(output_feed, input_feed)
        return outputs

    def save(self, sess, path, var_list=None, global_step=None):
        # var_list = None returns the list of all saveable variables
        saver = tf.train.Saver(var_list)

        # temporary code
        #del tf.get_collection_ref('LAYER_NAME_UIDS')[0]
        save_path = saver.save(sess, save_path=path, global_step=global_step)
        print('model saved at %s' % save_path)

    def restore(self, sess, save_path):
        ckpt = tf.train.get_checkpoint_state(save_path)
        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):
             print('restore model!')
             saver = tf.train.Saver()
             saver.restore(sess, ckpt.model_checkpoint_path)
        else:
            if not os.path.exists(save_path):
                os.makedirs(save_path)
            sess.run(tf.global_variables_initializer()) 
 
def define():
     config = {}
     config['save_path'] = 'model/detector/'
     config['cell_type'] = 'lstm'
     config['depth'] = 1
     config['source_vocabulary'] = 'data/voc.json'
     config['use_fp16'] = False
     config['class_num'] = 2
     config['use_dropout'] = True
     config['input'] = 'data/train.seq'
     config['valid'] = 'data/valid.seq'
     config['batch_size'] = 3
     config['voc'] = utils.load_voc(config['source_vocabulary'])
     config['hidden_units'] = 128
     config['use_residual'] = False
     config['optimizer'] = 'adam'
     config['learning_rate'] = 0.001
     config['max_gradient_norm'] = 1.0
     config['max_epochs'] = 10000
     config['big_num'] = 50
     return config

def train():
     config = define()

     from data.data_iterator import TextIterator     
     test_set = TextIterator(source=config['input'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'],
                            maxlen=15000)
     valid_set = TextIterator(source=config['valid'],
                            source_dict=config['source_vocabulary'],
                            maxlen=15000)

     with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True))) as sess:
       model = Detector(config, 'pretrain')
       model.restore(sess, config['save_path'])
       for epoch_idx in range(config['max_epochs']):
         for idx, sources in enumerate(test_set):
              source_seq = sources[0]
              label = sources[1]
              print(source_seq)
              exit()
              source, source_len = prepare_batch(source_seq)
              loss, acc, pred, logit, summary, _, labels, _1= model.pretrain(sess, source, label)
              #print("softmax_loss {}, acc {:g}, prob {}, pred {}, embed {}".format(loss, acc, logit, pred, embed))
              #print(_1, model.hidden_units, labels, pred)
              print("step {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), acc, loss))
              if (model.global_step.eval() % 50 == 0) or True:
                 _acc = 0
                 _loss = 0
                 _num = 0
                 for _, val_sources in enumerate(valid_set):
                        source_seq = val_sources[0]
                        label = val_sources[1]
                        source, source_len = prepare_batch(source_seq)
                        loss, acc, pred, logit = model.eval(sess, source, label)
                        _acc += acc * len(source_seq)
                        _loss += loss * len(label)
                        _num += len(source_seq)
                        print("step {}, acc {:g}, softmax_loss {:g}\npred {}\nlabel {}".format(model.global_step.eval(), acc, loss, pred, label))
                 print("step {}, acc {:g}, softmax_loss {:g}".format(model.global_step.eval(), _acc/_num, _loss))
              if (model.global_step.eval() % 50 == 0):
                 checkpoint_path = os.path.join(config['save_path'], 'detector')
                 model.save(sess, checkpoint_path, global_step=model.global_step)
                 json.dump(model.config,open('%s-%d.json' % (checkpoint_path, model.global_step.eval()), 'w'),indent=2)


if __name__ == '__main__':
     train()
