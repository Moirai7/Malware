#!/usr/bin/env python
# coding: utf-8

import os
import math
import time
import json
import random
from sklearn import svm
from sklearn import tree
from sklearn import preprocessing
from sklearn import metrics
from sklearn import decomposition
from sklearn.naive_bayes import BernoulliNB
from sklearn.ensemble import RandomForestClassifier

from collections import OrderedDict

import numpy as np
# import tensorflow as tf

from data.data_iterator import TextIterator

import data.util as util
import data.data_utils as data_utils
from data.data_utils import prepare_batch
import pickle

np.set_printoptions(threshold = np.nan)

def define():
    config = {}
    config['input'] = 'data/train.seq'
    config['valid'] = 'data/valid.seq'
    config['batch_size'] = 1000
    config['source_vocabulary'] = 'data/voc.json'
    config['maxlen']=600
    config['stride']=300
    config['max_batch']=900
    return config

config = define()
method = 'random forest'
 
def train():

    print('laiyao')
    print('method', method)
    train_set = TextIterator(source=config['input'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'])
    
    X = []
    y = []
    for idx, sources in enumerate(train_set):
        source_seq = sources[0]
        label = sources[1]
        sources, labels = prepare_batch(source_seq, label,max_batch=config['max_batch'], maxlen=config['maxlen'],stride = config['stride'],batch_size=config['batch_size'])
        for source,label in zip(sources, labels):
        # 由于采用SVM方法，只能取出一个batch
            # print ('label', label)
            for one_window_seq, one_window_label in zip(source, label):
                # print('len(one_window_seq)', len(one_window_seq))
                # print('len(one_window_label)', len(one_window_label))
                for one_window in one_window_seq:
                    X.append(one_window)
                    y.append(one_window_label)
        #     break
        # break
    
    X = np.array(X)   
    y = np.array(y)
    y = y.astype(float)
    print('X shape',X.shape)
    print('y shape',y.shape)
    # print('y',y)
    # print(y)
    
    # One_hot编码
    print('X max',np.max(X))
    enc = preprocessing.OneHotEncoder(n_values = np.max(X)+1)
    enc.fit(X)
    X = enc.transform(X)
    print('X_one_hot_shape',X.shape)
    
    # PCA降维
    pca=decomposition.TruncatedSVD(n_components = 300)
    pca.fit(X)
    X = pca.transform(X)
    print('X_pca_shape',X.shape)
    
    if method == 'svm': 
        clf = svm.SVC()
        clf.fit(X,y)
        # save model for future use
        with open('svm.pickle', 'wb') as fw:
            pickle.dump(clf, fw)
    elif method == 'decision tree':
        clf = tree.DecisionTreeClassifier()
        clf.fit(X,y)
        # save model for future use
        with open('decision-tree.pickle', 'wb') as fw:
            pickle.dump(clf, fw)
    elif method == 'naive bayes':
        clf = BernoulliNB()
        clf.fit(X,y)
        # save model for future use
        with open('naive-bayes.pickle', 'wb') as fw:
            pickle.dump(clf, fw)
    elif method == 'random forest':
        clf = RandomForestClassifier(n_estimators = 10)
        clf.fit(X,y)
        # save model for future use
        with open('random-forest.pickle', 'wb') as fw:
            pickle.dump(clf, fw)
    predict_y = clf.predict(X)
    # print(predict_y)
    print('train acc', metrics.accuracy_score(y,predict_y))
    return clf, enc, pca


def test(clf, enc, pca):
    valid_set = TextIterator(source=config['valid'],
                            batch_size=config['batch_size'],
                            source_dict=config['source_vocabulary'])
    predict_test_y = []
    truth_y = []
    for idx, sources in enumerate(valid_set):
        source_seq = sources[0]
        label = sources[1]
        sources, labels = prepare_batch(source_seq, label, max_batch=config['max_batch'], maxlen=config['maxlen'],stride = config['stride'],batch_size=config['batch_size'])
        for source,label in zip(sources, labels):
            for one_window_seq, one_window_label in zip(source, label):
                test_X = []
                for one_window in one_window_seq:
                    test_X.append(one_window)
                test_X = enc.transform(test_X)
                test_X = pca.transform(test_X)
                predict_tmp = clf.predict(test_X)
                predict_test_y_single = np.max(predict_tmp)
                predict_test_y.append(predict_test_y_single)
                truth_y.append(float(one_window_label))

    print(predict_test_y)
    
    print('test acc', metrics.accuracy_score(truth_y,predict_test_y))
    print('test pre', metrics.precision_score(truth_y,predict_test_y))
    print('test rec', metrics.recall_score(truth_y,predict_test_y))


if __name__ == '__main__':
    clf, enc, pca = train()
    test(clf, enc, pca)
