import numpy as np

import gzip
from data.util import load_dict

# Extra vocabulary symbols
PAD = '_PAD'
_GO = '_GO'
EOS = '_EOS' # also function as PAD
UNK = '_UNK'

extra_tokens = [PAD, _GO, EOS, UNK]

pad_token = extra_tokens.index(PAD)
start_token = extra_tokens.index(_GO)	# start_token = 0
end_token = extra_tokens.index(EOS)	# end_token = 1
unk_token = extra_tokens.index(UNK)

def fopen(filename, mode='r'):
    if filename.endswith('.gz'):
        return gzip.open(filename, mode)
    return open(filename, mode)


def load_inverse_dict(dict_path):
    orig_dict = load_dict(dict_path)
    idict = {}
    for words, idx in orig_dict.iteritems():
        idict[idx] = words
    return idict

def seq2words(seq, inverse_target_dictionary):
    words = []
    for w in seq:
        if w == end_token:
            break
        if w in inverse_target_dictionary:
            words.append(inverse_target_dictionary[w])
        else:
            words.append(UNK)
    return ' '.join(words)
import random
# batch preparation of a given sequence
def prepare_batch(seqs_x, stride=None, maxlen=None, batch_size=None):
    maxlen = int(maxlen)
    stride = int(stride)
    while len(seqs_x) < batch_size:
        _add = (batch_size-len(seqs_x))%len(seqs_x)
        if _add == 0:
           _t = list(range(0, len(seqs_x))) * int((batch_size-len(seqs_x))/len(seqs_x))
        else:
           _t = random.sample(range(0, len(seqs_x)), _add)
        seqs_x.extend(seqs_x[i] for i in _t)

    res = []
    lengths_x = []
    for ss in seqs_x:
        _ss = []
        diff = maxlen - len(ss) % stride
        if maxlen and len(ss) % stride != 0:
            ss.extend([end_token])
            if diff > 1:
               ss.extend([pad_token]*(diff-1))
        for i in range(0, len(ss)-maxlen+1, stride):
            _ss.append(ss[i: i+maxlen])
            if (ss[i+maxlen-1] != pad_token and ss[i+maxlen-1] != end_token):
               lengths_x.append(maxlen)
            else:
               lengths_x.append(maxlen-diff)
        res.append(_ss)
    
    return np.array(res), np.array(lengths_x)

# batch preparation of a given sequence pair for training
def prepare_train_batch(seqs_x, seqs_y, maxlen=None):
    # seqs_x, seqs_y: a list of sentences
    lengths_x = [len(s) for s in seqs_x]
    lengths_y = [len(s) for s in seqs_y]

    if maxlen is not None:
        new_seqs_x = []
        new_seqs_y = []
        new_lengths_x = []
        new_lengths_y = []
        for l_x, s_x, l_y, s_y in zip(lengths_x, seqs_x, lengths_y, seqs_y):
            if l_x <= maxlen and l_y <= maxlen:
                new_seqs_x.append(s_x)
                new_lengths_x.append(l_x)
                new_seqs_y.append(s_y)
                new_lengths_y.append(l_y)
        lengths_x = new_lengths_x
        seqs_x = new_seqs_x
        lengths_y = new_lengths_y
        seqs_y = new_seqs_y

        if len(lengths_x) < 1 or len(lengths_y) < 1:
            return None, None, None, None

    batch_size = len(seqs_x)
    
    x_lengths = np.array(lengths_x)
    y_lengths = np.array(lengths_y)

    maxlen_x = np.max(x_lengths)
    maxlen_y = np.max(y_lengths)

    x = np.ones((batch_size, maxlen_x)).astype('int32') * end_token
    y = np.ones((batch_size, maxlen_y)).astype('int32') * end_token
    for idx, [s_x, s_y] in enumerate(zip(seqs_x, seqs_y)):
        x[idx, :lengths_x[idx]] = s_x
        y[idx, :lengths_y[idx]] = s_y
    return x, x_lengths, y, y_lengths
